{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Custom training with tf.distribute.Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM6W__qraV55"
   },
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7NsM-Bma5wNw"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [],
   "source": [
    "splits = ['train[:80%]', 'train[80%:90%]', 'train[90%:]']\n",
    "\n",
    "(train_examples, validation_examples, test_examples), info = tfds.load('oxford_flowers102', with_info=True, as_supervised=True, split = splits, data_dir='data/')\n",
    "\n",
    "num_examples = info.splits['train'].num_examples\n",
    "num_classes = info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AXoHhrsbdF3"
   },
   "source": [
    "## Create a strategy to distribute the variables and the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F2VeZUWUj5S4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZngeM_2o0_JO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k53F5I_IiGyI"
   },
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jwJtsCQhHK-E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data/resnet_50_feature_vector with input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = num_examples\n",
    "EPOCHS = 10\n",
    "pixels = 224\n",
    "MODULE_HANDLE = 'data/resnet_50_feature_vector'\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RHGFit478BWD"
   },
   "outputs": [],
   "source": [
    "def format_image(image, label):\n",
    "    image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
    "    return  image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the global batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_batch_size(batch_size_per_replica, strategy):\n",
    "    global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync    \n",
    "    return global_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "GLOBAL_BATCH_SIZE = set_global_batch_size(BATCH_SIZE_PER_REPLICA, strategy)\n",
    "\n",
    "print(GLOBAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WYrMNNDhAvVl"
   },
   "outputs": [],
   "source": [
    "train_batches = train_examples.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE_PER_REPLICA).prefetch(1)\n",
    "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE_PER_REPLICA).prefetch(1)\n",
    "test_batches = test_examples.map(format_image).batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the distributed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_datasets(strategy, train_batches, validation_batches, test_batches):\n",
    "    train_dist_dataset = strategy.experimental_distribute_dataset(train_batches)\n",
    "    val_dist_dataset = strategy.experimental_distribute_dataset(validation_batches)\n",
    "    test_dist_dataset = strategy.experimental_distribute_dataset(test_batches)    \n",
    "    return train_dist_dataset, val_dist_dataset, test_dist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist_dataset, val_dist_dataset, test_dist_dataset = distribute_datasets(strategy, train_batches, validation_batches, test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.distribute.input_lib.DistributedDataset'>\n",
      "<class 'tensorflow.python.distribute.input_lib.DistributedDataset'>\n",
      "<class 'tensorflow.python.distribute.input_lib.DistributedDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dist_dataset))\n",
    "print(type(val_dist_dataset))\n",
    "print(type(test_dist_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is a tuple that contains 2 values \n",
      "x[0] contains the features, and has shape (64, 224, 224, 3)\n",
      "  so it has 64 examples in the batch, each is an image that is (224, 224, 3)\n",
      "x[1] contains the labels, and has shape (64,)\n"
     ]
    }
   ],
   "source": [
    "x = iter(train_dist_dataset).get_next()\n",
    "    \n",
    "print(f\"x is a tuple that contains {len(x)} values \")\n",
    "print(f\"x[0] contains the features, and has shape {x[0].shape}\")\n",
    "print(f\"  so it has {x[0].shape[0]} examples in the batch, each is an image that is {x[0].shape[1:]}\")\n",
    "print(f\"x[1] contains the labels, and has shape {x[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAXAo_wWbWSb"
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9ODch-OFCaW4"
   },
   "outputs": [],
   "source": [
    "class ResNetModel(tf.keras.Model):\n",
    "    def __init__(self, classes):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self._feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
    "                                                 trainable=False) \n",
    "        self._classifier = tf.keras.layers.Dense(classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self._feature_extractor(inputs)\n",
    "        x = self._classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9iagoTBfijUz"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-wlFFZbP33n"
   },
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "R144Wci782ix"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    def compute_loss(labels, predictions):\n",
    "        per_example_loss = loss_object(labels, predictions)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "    \n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8y54-o9T2Ni"
   },
   "source": [
    "## Define the metrics to track loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zt3AHb46Tr3w"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        name='train_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuKuNXPORfqJ"
   },
   "source": [
    "## Instantiate the model, optimizer, and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OrMmakq5EqeQ"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = ResNetModel(classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zUQ_nAP1MtA9"
   },
   "outputs": [],
   "source": [
    "def train_test_step_fns(strategy, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy):\n",
    "    with strategy.scope():\n",
    "        def train_step(inputs):\n",
    "            images, labels = inputs\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(images, training=True)\n",
    "                loss = compute_loss(labels, predictions)\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            train_accuracy.update_state(labels, predictions)\n",
    "            return loss \n",
    "\n",
    "        def test_step(inputs):\n",
    "            images, labels = inputs\n",
    "            \n",
    "            predictions = model(images, training=False)\n",
    "            t_loss = compute_loss(labels, predictions)\n",
    "\n",
    "            test_loss.update_state(t_loss)\n",
    "            test_accuracy.update_state(labels, predictions)\n",
    "        \n",
    "        return train_step, test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step, test_step = train_test_step_fns(strategy, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When passing in args=list_of_inputs:\n",
      "number of arguments passed is 2\n",
      "\n",
      "When passing in args=(list_of_inputs)\n",
      "number of arguments passed is 2\n",
      "\n",
      "When passing in args=(list_of_inputs,)\n",
      "number of arguments passed is 1\n"
     ]
    }
   ],
   "source": [
    "def fun1(args=()):\n",
    "    print(f\"number of arguments passed is {len(args)}\")\n",
    "    \n",
    "    \n",
    "list_of_inputs = [1,2]\n",
    "print(\"When passing in args=list_of_inputs:\")\n",
    "fun1(args=list_of_inputs)\n",
    "print()\n",
    "print(\"When passing in args=(list_of_inputs)\")\n",
    "fun1(args=(list_of_inputs))\n",
    "print()\n",
    "print(\"When passing in args=(list_of_inputs,)\")\n",
    "fun1(args=(list_of_inputs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_train_test_step_fns(strategy, train_step, test_step, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy):\n",
    "    with strategy.scope():\n",
    "        @tf.function\n",
    "        def distributed_train_step(dataset_inputs):\n",
    "            per_replica_losses = strategy.run(train_step,args=(dataset_inputs,))\n",
    "            return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                                   axis=None)\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_test_step(dataset_inputs):\n",
    "            return strategy.run(test_step, args=(dataset_inputs,))\n",
    "    \n",
    "        return distributed_train_step, distributed_test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_train_step, distributed_test_step = distributed_train_test_step_fns(strategy, train_step, test_step, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the distributed training in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "gX975dMSNw0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:51,  8.54s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.595747947692871, Accuracy: 6.740196228027344, Test Loss: 0.05948949232697487, Test Accuracy: 15.686275482177734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:42,  7.87s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.53836989402771, Accuracy: 50.98039627075195, Test Loss: 0.04145458713173866, Test Accuracy: 48.039215087890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:40,  7.73s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.4094295501708984, Accuracy: 85.6617660522461, Test Loss: 0.032817449420690536, Test Accuracy: 62.74510192871094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:43,  7.97s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.822101891040802, Accuracy: 94.9754867553711, Test Loss: 0.027745109051465988, Test Accuracy: 62.74510192871094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:41,  7.77s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.5378504991531372, Accuracy: 97.30392456054688, Test Loss: 0.024688323959708214, Test Accuracy: 64.70588684082031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:44,  8.05s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.3714151680469513, Accuracy: 98.4068603515625, Test Loss: 0.023377155885100365, Test Accuracy: 65.68627166748047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:43,  7.93s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.2776481807231903, Accuracy: 99.38725280761719, Test Loss: 0.021725326776504517, Test Accuracy: 66.66667175292969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:44,  8.04s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.21669873595237732, Accuracy: 99.75489807128906, Test Loss: 0.02106780931353569, Test Accuracy: 69.60784149169922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:43,  7.96s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.17298102378845215, Accuracy: 99.87745666503906, Test Loss: 0.02041405811905861, Test Accuracy: 71.5686264038086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:39,  7.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.14275483787059784, Accuracy: 99.87745666503906, Test Loss: 0.019934987649321556, Test Accuracy: 68.62745666503906\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for x in tqdm(train_dist_dataset):\n",
    "            total_loss += distributed_train_step(x)\n",
    "            num_batches += 1\n",
    "        train_loss = total_loss / num_batches\n",
    "\n",
    "        for x in test_dist_dataset:\n",
    "            distributed_test_step(x)\n",
    "\n",
    "        template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "                    \"Test Accuracy: {}\")\n",
    "        print (template.format(epoch+1, train_loss,\n",
    "                               train_accuracy.result()*100, test_loss.result(),\n",
    "                               test_accuracy.result()*100))\n",
    "\n",
    "        test_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEaNCzYQvFqo"
   },
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "1zAlTlRxrqFu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tmp/mymodel/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tmp/mymodel/1/assets\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"./tmp/mymodel/1/\"\n",
    "tf.saved_model.save(model, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0Zfmx6LvTJA"
   },
   "source": [
    "## Zip the SavedModel Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "gMuo2wQls41l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))\n",
    "\n",
    "zipf = zipfile.ZipFile('./mymodel.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir('./tmp/mymodel/1/', zipf)\n",
    "zipf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "##### Coursera: Custom and Distributed Training with TensorFlow [course](https://www.coursera.org/learn/custom-distributed-training-with-tensorflow)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ExerciseAnswer.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "TF3C2W4-1",
    "TF3C2W4-2",
    "TF3C2W4-3",
    "TF3C2W4-4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
